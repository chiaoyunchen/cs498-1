---
title: "Homework 2"
author: "CS 498, Spring 2018, Xiaoming Ji"
date: ''
output:
  html_document:
    toc: yes
  pdf_document: default
---

The UC Irvine machine learning data repository hosts a collection of data on adult income, donated by Ronny Kohavi and Barry Becker. You can find this data at https://archive.ics.uci.edu/ml/datasets/Adult For each record, there is a set of continuous attributes, and a class "less than 50K" or "greater than 50K". There are 48842 examples. You should use only the continuous attributes (see the description on the web page) and drop examples where there are missing values of the continuous attributes. Separate the resulting dataset randomly into 10% validation, 10% test, and 80% training examples.

Write a program to train a support vector machine on this data using stochastic gradient descent. We will ignore the id number, and use the continuous variables as a feature vector. You should scale these variables so that each has unit variance. We will search for an appropriate value of the regularization constant, trying at least the values [1e-3, 1e-2, 1e-1, 1]. Use the validation set for this search. We will use at least 50 epochs of at least 300 steps each. In each epoch, we will separate out 50 training examples at random for evaluation (call this the set held out for the epoch). We will compute the accuracy of the current classifier on the set held out for the epoch every 30 steps. We will produce:

- A plot of the accuracy every 30 steps, for each value of the regularization constant.
- A plot of the magnitude of the coefficient vector every 30 steps, for each value of the regularization constant.
- Estimate of the best value of the regularization constant, together with a brief description of why you believe that is a good value.
- Estimate of the accuracy of the best classifier on the 10% test dataset data.

```{r}
library(readr)
set.seed(19720816)

adult_data = read.csv("./data/adult.data", header = FALSE)[,c(1, 3, 5, 11, 12, 13, 15)]
adult_data[, 7] = as.numeric(adult_data[, 7])
adult_data[adult_data[, 7] == 1, 7] = -1
adult_data[adult_data[, 7] == 2, 7] = 1

#Normalize the data
for (i in 1:6) {
  s = sd(adult_data[,i])
  m = mean(adult_data[,i])
  
  adult_data[,i] = (adult_data[,i] - m) / s
}

#Calculate train/validation/test split
total_count = dim(adult_data)[1]
test_count = round(total_count * 0.1)
valid_count = round(total_count * 0.1)

test_indexes = sample(1:total_count,test_count)
```

$\lambda$

$$
\beta^TX + \beta_0
$$

```{r}
sgd = function(epoch, steps, sample_count, validation_count, data, test_data, beta, lambda) {
  feature_count = dim(data)[2] - 1
  accuracy = c()
  w = c()
  
  for (epoch in 1:epoch) {
    train_indexes = sample(1: dim(data)[1], dim(data)[1] - validation_count)
    train_data  = data[train_indexes, ]
    valid_data  = data[-train_indexes,]
    steplength  = 1 / (0.01 * epoch + 50)
    
    for (step in 1:steps){
      sample_indexes = sample(1:dim(train_data)[1], sample_count)
      x = train_data[sample_indexes, 1:feature_count]
      y = train_data[sample_indexes, feature_count + 1]
      
      h = y * (beta[1:feature_coun] %*% t(x) + beta[feature_count + 1])
     
      beta_g = matrix(0, length(y), length(x))
      beta_g[h >=1] = lambda * beta[1:feature_coun]
      beta_g[h < 1] = lambda * beta[1:feature_coun] - y[h < 1] * x
      beta[1:feature_count] = beta[1:feature_count]  - steplength * sum (beta_g)

      beta0_g = y * 0
      beta0_g[beta0_g < 1] = y[beta0_g < 1]
      beta[feature_count + 1] = beta[feature_count + 1] - steplength * sum(beta0_g)
    }
  }

}

max_epoch = 50
max_step  = 300
sample_count = 50

lambdas =  c(1e-3, 1e-2, 1e-1, 1)
betas = runif(6)

sgd(1, 300, 50, valid_count, adult_data[-test_indexes], adult_data[test_indexes], betas, lambdas[1])
```

