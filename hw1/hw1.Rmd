---
title: "Homework 1 Problem 1"
author: "CS 498, Spring 2018, Xiaoming Ji"
date: ''
output:
  html_document:
    toc: yes
  pdf_document: default
---



##Part A
Build a simple naive Bayes classifier to classify this data set. We will use 20% of the data for evaluation and the other 80% for training. There are a total of 768 data-points.

We use a normal distribution to model each of the class-conditional distributions. 

```{r}
#Prepare training and testing data
library(readr)
data  = read.csv("pima-indians-diabetes.data.csv", header = FALSE)
all_x = data[,-c(9)]
all_y = data[,9]
```

We define a function to train naive Byaes model by calculating p(y=class) and mean and standard deviation of feature variables for each class.

```{r}
gnb.fit = function(features, classes) {
  model     = list("p_y", "mu","sigma")
  class_val = unique(classes)
  class_len = length(class_val)
  y_len     = length(classes)

  p_y       = rep(0, class_len)
  mu        = array(0, c(class_len, dim(features)[2]))
  sigma     = array(0, c(class_len, dim(features)[2]))
  
  i = 1
  for (y in class_val) {
    indexes = classes == y
    p_y[i]  = length(classes[indexes]) / y_len
    
    x         = features[indexes,]
    mu[i,]    = sapply(x, mean, na.rm=TRUE)
    sigma[i,] = sapply(x, sd, na.rm=TRUE)

    i = i + 1 
  }
  
  model$p_y = p_y
  model$mu = mu
  model$sigma = sigma
  
  return (model)
}
```

We will use formula,
$$
\sum_jlog(x_j|y_k) + logp(y_k)
$$ 
to calculate the log probablity of one class $y_k$. Since we use normal distribution for each feature $x_j$, the probability of $x_j$ given class $y_k$ thus is,
$$
p(x_j|y_k) = \frac{1}{{\sigma_k \sqrt {2\pi }}}e^-\frac{{\left({x - \mu_k} \right)^2}}{2\sigma_k^2}
$$
where $\mu_k$ and $\sigma_k$ are mean and stanstard deviation of feature $x_j$ for a given class $y_k$. They are calculated by function *gnb.fit*.

We define a function to predict the class given a model and features.
```{r}
gnb.predict = function(model, x) {
  class_len = length(model$p_y)
  y_score = array(0, c(dim(x)[1],class_len))
  
  for (i in 1:class_len) {
    scale       = (t(x)-model$mu[i,])/model$sigma[i,]
    log_score   = (-1/2) * scale ^ 2 - log(model$sigma[i,] * sqrt(2*pi))
    y_score[,i] = colSums(log_score, na.rm = TRUE) + log(model$p_y[i])
  }
  
  return (max.col(y_score) - 1)
}
```

Let's train and check the accuracy of the classifier on the 20% evaluation data.

```{r}
set.seed(19720816)
train_len = round(dim(all_x)[1] * 0.8)
train_index = sample(1:dim(all_x)[1], round(dim(all_x)[1] * 0.8))

#train_x = all_x[train_index,]
#train_y = all_y[train_index]
#eval_x = all_x[-train_index,]
#eval_y = all_y[-train_index]

train_x = all_x[1:614,]
train_y = all_y[1:614]
eval_x = all_x[615:768,]
eval_y = all_y[615:768]

model = gnb.fit(train_x, train_y)
pred_y = gnb.predict(model, eval_x)

accuracy = sum(pred_y == eval_y)/length(eval_y)
```

We got accuracy: $`r accuracy`$

##Part B
We will adjust our code so that, for attribute 3 (Diastolic blood pressure), attribute 4 (Triceps skin fold thickness), attribute 6 (Body mass index), and attribute 8 (Age), it regards a value of 0 as a missing value when estimating the class-conditional distributions, and the posterior. R uses a special number NA to flag a missing value. 

```{r}
all_x_na = all_x

for (i in c(3, 5, 6, 8))
{
  index = all_x_na[, i]==0
  all_x_na[index, i] = NA
}

train_x = all_x_na[1:614,]
train_y = all_y[1:614]
eval_x = all_x_na[615:768,]
eval_y = all_y[615:768]
```

```{r}
model = gnb.fit(train_x, train_y)
pred_y = gnb.predict(model, eval_x)

accuracy = sum(pred_y == eval_y)/length(eval_y)
```
We got accuracy: $`r accuracy`$