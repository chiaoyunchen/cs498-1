---
title: "Homework 1"
author: "CS 498, Spring 2018, Xiaoming Ji"
date: ''
output:
  html_document:
    toc: yes
  pdf_document: default
---


#Problem 1

##Part A
Build a simple naive Bayes classifier to classify this data set. We will use 20% of the data for evaluation and the other 80% for training. There are a total of 768 data-points.

We use a normal distribution to model each of the class-conditional distributions. 

```{r}
#Prepare training and testing data
library(readr)
all_data  = read.csv("pima-indians-diabetes.data.csv", header = FALSE)
```

We define a function to train naive Byaes model by calculating p(y=class) and mean and standard deviation of feature variables for each class.

```{r}
#Gaussian model
gnb.fit = function(features, labels) {
  model      = list("p_y", "mu","sigma")
  class_val  = unique(labels)
  class_num  = max(class_val) + 1
  record_num = length(labels)

  p_y       = rep(0, class_num)
  mu        = matrix(0, class_num, dim(features)[2])
  sigma     = matrix(0, class_num, dim(features)[2])
  
  for (y in class_val) {
    indexes = labels == y
    p_y[y + 1]  = length(labels[indexes]) / record_num
    
    x             = features[indexes,]
    mu[y + 1,]    = sapply(x, mean, na.rm=TRUE)
    sigma[y + 1,] = sapply(x, sd, na.rm=TRUE)
  }
  
  model$p_y = p_y
  model$mu = mu
  model$sigma = sigma
  
  return (model)
}

#Bernoulli model
bnb.fit = function(features, labels) {
  model = list("p_y", "p_x")
  class_val  = unique(labels)
  class_num  = max(class_val) + 1
  record_num = length(labels)
  
  p_y = rep(0, class_num)
  p_x = matrix(0, class_num, dim(features)[2])
  
  for (y in class_val){
    indexes = labels == y
    p_y[y + 1]  = length(labels[indexes]) / record_num
    
    x           = features[indexes,]
    p_x[y + 1,] = colSums(x > 0) / length(indexes)
  }
  
  model$p_y = p_y
  model$p_x = p_x
  
  return (model)
}
```

Since we use normal distribution for each feature $x_j$, the probability of $x_j$ given class $y_k$ is given as,
$$
p(x_j|y_k) = \frac{1}{{\sigma_k \sqrt {2\pi }}}e^-\frac{{\left({x - \mu_k} \right)^2}}{2\sigma_k^2}
$$
where $\mu_k$ and $\sigma_k$ are mean and stanstard deviation of feature $x_j$ for a given class $y_k$. They are calculated by function *gnb.fit*.

To calculate the log probablity of one class $y_k$, we use formula,
$$
\sum_jlog(x_j|y_k) + logp(y_k) 
$$ 
$$
= -\sum_j(log\sigma_k + log\sqrt{2\pi} + \frac{\left(x - \mu_k\right)^2}{2\sigma_k^2}) + logp(y_k)
$$
Given $log\sqrt{2\pi}$ is contant, we have,
$$
\propto -\sum_j(log\sigma_k+ \frac{\left(x - \mu_k\right)^2}{2\sigma_k^2}) + logp(y_k)
$$ 

We define a function to predict the class given a model and features.
```{r}
gnb.predict = function(model, x) {
  class_num = length(model$p_y)
  y_score = matrix(0, dim(x)[1],class_num)
  
  for (i in 1:class_num) {
    scale       = (t(x)-model$mu[i,])/model$sigma[i,]
    logs        = -(log(model$sigma[i,]) + (1/2) * scale ^ 2)
    y_score[,i] = colSums(logs, na.rm = TRUE) + log(model$p_y[i])
  }
  
  return (max.col(y_score) - 1)
}

bnb.predict = function(model, x) {
    class_num = length(model$p_y)
    y_score = matrix(0, dim(x)[1], class_num)
    for (i in 1:class_num){
      logs = t(x) * log(model$p_x[i,]) + t(1 - x) * log(1 - model$p_x[i,])
      y_score[,i] =  colSums(logs, na.rm = TRUE)+ log(model$p_y[i])
    }
    
    return (max.col(y_score) - 1)
}
```

Let's train and check the accuracy of the classifier on the 20% evaluation data. We will do 10-fold cross-validation and select the best model for testing evaluation data.

```{r}
#Cross validate and return best model
cross_validation = function(x, y, fold_num, model = "Gaussian") {
  func_map = list("Gaussian" = list(fit = gnb.fit, predict = gnb.predict), 
                  "Bernoulli" = list(fit = bnb.fit, predict = bnb.predict))
  
  train_num = length(y)
  accuracy  = rep(0, fold_num)
  models    = list()


  valid_num = round(train_num / fold_num)
  for (i in 1:fold_num){
    index_from = (i - 1) * valid_num + 1
    index_to   = i * valid_num + 1
    if(index_to > train_num) index_to = train_num
                                                           
    valid_indexes = index_from : index_to
    train_x = x[-valid_indexes, ]
    train_y = y[-valid_indexes]
    valid_x = x[valid_indexes, ]
    valid_y = y[valid_indexes]
    
    models[[i]] = func_map[[model]]$fit(train_x, train_y)
    pred_y = func_map[[model]]$predict(models[[i]], valid_x)
    
    accuracy[i] = sum(pred_y == valid_y) / length(valid_y)
    #print(accuracy[i])
  }
  
  #print(which.max(accuracy))
  return (models[[which.max(accuracy)]])
}

set.seed(19720816)
fold_num = 10
train_indexes = sample(1:dim(all_data)[1], round(dim(all_data)[1] * 0.8))

best_model = cross_validation(all_data[train_indexes,-c(9)], all_data[train_indexes,c(9)], 10)

test_x = all_data[-train_indexes,-c(9)]
test_y = all_data[-train_indexes,c(9)]
pred_y = gnb.predict(best_model, test_x)
best_accuracy = sum(pred_y == test_y)/length(test_y)
```

We got accuracy: $`r best_accuracy`$

##Part B
We will adjust our code so that, for attribute 3 (Diastolic blood pressure), attribute 4 (Triceps skin fold thickness), attribute 6 (Body mass index), and attribute 8 (Age), it regards a value of 0 as a missing value when estimating the class-conditional distributions, and the posterior. R uses a special number NA to flag a missing value. 

```{r}
all_x_na = all_data

for (i in c(3, 5, 6, 8))
{
  index = all_x_na[, i]==0
  all_x_na[index, i] = NA
}

set.seed(19720816)
fold_num = 10
train_indexes = sample(1:dim(all_x_na)[1], round(dim(all_x_na)[1] * 0.8))

best_model = cross_validation(all_x_na[train_indexes,-c(9)], all_x_na[train_indexes,c(9)], 10)

test_x = all_x_na[-train_indexes,-c(9)]
test_y = all_x_na[-train_indexes,c(9)]
pred_y = gnb.predict(best_model, test_x)
best_accuracy = sum(pred_y == test_y)/length(test_y)
```

We got accuracy: $`r best_accuracy`$. The result don't have much difference compared with the previous one.

##Part C
Use the caret and klaR packages to build a naive bayes classifier for this data, assuming that no attribute has a missing value. We will do 10-fold cross-validation and test the accuracy of the classifier on the held out 20%.

```{r, message=FALSE, warning=FALSE}
library(caret)

train_indexes = createDataPartition(y = all_data[, 9], p = 0.8, list = FALSE)
train_x = all_data[train_indexes,-c(9)]
train_y = as.factor(all_data[train_indexes,c(9)])
model   = train(train_x, train_y, 'nb', trControl = trainControl(method='cv', number=10))

test_x = all_data[-train_indexes,-c(9)]
test_y = all_data[-train_indexes,c(9)]
pred_y = predict(model, newdata = test_x)
accuracy = sum(pred_y == test_y)/length(test_y)
```
We got accuracy: $`r accuracy`$

##Part D
Now we use SVMLight to train and evaluate an SVM to classify this data.

```{r, message=FALSE}
library(klaR)

train_indexes = createDataPartition(y = all_data[, 9], p = 0.8, list = FALSE)
train_x = all_data[train_indexes,-c(9)]
train_y = all_data[train_indexes,c(9)]

model = svmlight(train_x, train_y, pathsvm="./svmlight")

test_x = all_data[-train_indexes,-c(9)]
test_y = all_data[-train_indexes,c(9)]
pred_y = predict(model, newdata = test_x)
accuracy = sum(pred_y$class == test_y)/length(test_y)
```

We got accuracy: $`r accuracy`$

#Problem 2
We make a function to construct a bounding box so that the horizontal (resp. vertical) range of ink pixels runs the full horizontal (resp. vertical) range of the box.

```{r}
#Stretch image to new size
stretch_image = function(img, new_width, new_height) {
  new_img = apply(img, 2, function(y){return (spline(y, n = new_height)$y)})
  new_img = t(apply(new_img, 1, function(y){return (spline(y, n = new_width)$y)}))

  new_img[new_img < 0] = 0
  new_img[new_img > 255] = 255
  new_img = round(new_img)
  
  return (new_img)
}

#Make stretched bounding box
sb_box = function(img, box_width, box_height) {
  col = colSums(img)
  row = rowSums(img)

  #Calculate image boundary and extract the bounding image
  top = bottom = left = right  = 0

  for (i in 1:length(row)) {if (row[i] != 0) { top = i; break}}
  for (i in length(row):1) {if (row[i] != 0) { bottom = i; break}}  
  for (i in 1:length(col)) {if (col[i] != 0) { left = i; break}}
  for (i in length(col):1) {if (col[i] != 0) { right = i; break}}

  if(top >= bottom || left >= right){stop("bad image")}
  
  box_img = img[top:bottom, left:right]
  
  
  top    = (dim(img)[1] - box_height) / 2 + 1
  bottom = top + box_height - 1
  left   = (dim(img)[2] - box_width) / 2 + 1
  right  = left + box_width - 1

  ss_img = array(0, dim(img))
  ss_img[top:bottom, left:right] = (stretch_image(box_img, box_width, box_height))
  
  #Draw box
  #ss_img[top, (left - 1) : (right + 1)]    = 255
  #ss_img[bottom + 1, (left - 1) : (right + 1)] = 255
  #ss_img[(top - 1): (bottom + 1), (left - 1)]    = 255
  #ss_img[(top - 1): (bottom + 1), (right + 1)]   = 255
  ss_img[top, left:right]    = 255
  ss_img[bottom, left: right] = 255
  ss_img[top: bottom, left]    = 255
  ss_img[top: bottom, right]   = 255
  
  #Thresholding
  #ss_img[ss_img > 128] = 255
  #ss_img[ss_img <= 128] = 0
  
  return (ss_img)
}
```

To improve performance for repetitive experiment, we process the original images and save the stretched bounding box images to files. 

```{r, eval=FALSE}
process_save = function(input_gzfile_name, output_file_name) {
  input_gzfile = file(input_gzfile_name, "rb")
  output_file = file(output_file_name, "wb")

  header = readBin(input_gzfile, integer(), n = 4,  endian = "big")
  writeBin(header, output_file, endian = "big")
  
  sb_images = list()
  original_images = list()
  
  for (i in 1:header[2]) {
    data = readBin(input_gzfile, integer(), size = 1, n = 28*28, endian = "big", 
                   signed = FALSE)
    if(length(data) != 28*28) break
    original_images[[i]] = matrix(data, 28, 28);
    
    sb_images[[i]] = sb_box(original_images[[i]], 20, 20)
    writeBin(as.integer(sb_images[[i]]), output_file, size = 1)
  }

  close(input_gzfile)
  close(output_file)
}

process_save("./MNIST/train-images-idx3-ubyte", "./MNIST/train-images-idx3-ubyte-sb")
process_save("./MNIST/t10k-images-idx3-ubyte", "./MNIST/t10k-images-idx3-ubyte-sb")
```

Load data from saved file.
```{r}
load_image_data = function(file_name){
  img_file = file(file_name, "rb")
  header = readBin(img_file, integer(), n = 4,  endian = "big")
  
  data = matrix(0, header[2], 28 * 28)
  #data = list()
  for (i in 1:header[2]) {
    img = readBin(img_file, integer(), size = 1, n = 28*28, endian = "big", 
                   signed = FALSE)
    if(length(img) != 28*28) break
    data[i,] = img
    #data[[i]] = img
  }
  close(img_file)
  
  return (data)
}

load_label_data = function(file_name){
  label_file = file(file_name, "rb")
  header = readBin(label_file, integer(), n = 2,  endian = "big")
  
  data = readBin(label_file, integer(), size = 1, n = header[2], endian = "big", 
                   signed = FALSE)
  close(label_file)
  
  return (data)
}

  train_data = load_image_data("./MNIST/train-images-idx3-ubyte")
  test_data = load_image_data("./MNIST/t10k-images-idx3-ubyte")
  sb_train_data = load_image_data("./MNIST/train-images-idx3-ubyte-sb")
  sb_test_data = load_image_data("./MNIST/t10k-images-idx3-ubyte-sb")
  train_label = load_label_data("./MNIST/train-labels-idx1-ubyte")
  test_label = load_label_data("./MNIST/t10k-labels-idx1-ubyte")
  
  #Threshhold image to make it binary value (ink and paper)
  bin_train_data = train_data
  bin_test_data = test_data
  bin_train_data[bin_train_data <= 128] = 0
  bin_train_data[bin_train_data > 128] = 1
  bin_test_data[bin_test_data <= 128] = 0
  bin_test_data[bin_test_data > 128] = 1
  
  sb_bin_train_data = sb_train_data
  sb_bin_test_data  = sb_test_data
  sb_bin_train_data[sb_bin_train_data <= 128] = 0
  sb_bin_train_data[sb_bin_train_data > 128] = 1
  sb_bin_test_data[sb_bin_test_data <= 128] = 0
  sb_bin_test_data[sb_bin_test_data > 128] = 1
```

Let's show some images for both untouched and stredched bounding box.
```{r, echo=FALSE}
par(mfrow=c(6,5))
par(mar = c(0,0,0,0))

index = sample(1:60000, 10)
for(i in index)
{
  image(matrix(train_data[i,], 28, 28)[, 28:1], col  = gray((255:0)/255))
}

for(i in index)
{
  image(matrix(sb_train_data[i,], 28, 28)[, 28:1], col  = gray((255:0)/255))
}

for(i in index)
{
  image(matrix(sb_bin_train_data[i,], 28, 28)[, 28:1], col  = gray((1:0)/1))
}
```


##Part A
Investigate classifying MNIST using naive Bayes. We use 20 x 20 for your bounding box dimensions.

```{r}
train_data = as.data.frame(train_data)
bin_train_data = as.data.frame(bin_train_data)
sb_train_data = as.data.frame(sb_train_data)
sb_bin_train_data = as.data.frame(sb_bin_train_data)

best_model = cross_validation(train_data, train_label, 10)
pred_y = gnb.predict(best_model, test_data)
gaussian_accuracy = sum(pred_y == test_label)/length(test_label)

best_model = cross_validation(bin_train_data, train_label, 10)
pred_y = gnb.predict(best_model, bin_test_data)
bin_gaussian_accuracy = sum(pred_y == test_label)/length(test_label)

best_model = cross_validation(sb_train_data, train_label, 10)
pred_y = gnb.predict(best_model, sb_test_data)
gaussian_sb_accuracy = sum(pred_y == test_label)/length(test_label)

best_model = cross_validation(sb_bin_train_data, train_label, 10)
pred_y = gnb.predict(best_model, sb_bin_test_data)
bin_gaussian_sb_accuracy = sum(pred_y == test_label)/length(test_label)

best_model = cross_validation(bin_train_data, train_label, 10, model = "Bernoulli")
pred_y = bnb.predict(best_model, bin_test_data)
bernoulli_accuracy = sum(pred_y == test_label)/length(test_label)

best_model = cross_validation(sb_bin_train_data, train_label, 10, model = "Bernoulli")
pred_y = bnb.predict(best_model, sb_bin_test_data)
bernoulli_sb_accuracy = sum(pred_y == test_label)/length(test_label)
```

The accuracy values for the six combinations of Gaussian (original and thresholding) v. Bernoulli distributions and untouched images v. stretched bounding boxes. 

|   Accuracy            | Gaussian                 |Gaussian (Thresholding)         | Bernoulli                 |
|-----------------------|--------------------------|-----------------------------|---------------------------|
|untouched images       | $`r gaussian_accuracy`$  | $`r bin_gaussian_accuracy`$| $`r bernoulli_accuracy`$ |
|stretched bounding box | $`r gaussian_sb_accuracy`$| $`r bin_gaussian_sb_accuracy`$ | $`r bernoulli_sb_accuracy`$|


- From the results, we can see Bernoulli distribution is better for untouched pixels and Gaussian distribution is better for stretched bounding box images.
- We also notice that Gaussian distribution used against (unthresholded) streched bounding box gives us the best result $`r gaussian_sb_accuracy`$.

##Part B
Investigate classifying MNIST using a decision forest. For this you should use a library. For your forest construction, try out and compare the combinations of parameters shown in the table (i.e. depth of tree, number of trees, etc.) by listing the accuracy for each of the following cases: untouched raw pixels; stretched bounding box. Please use 20 x 20 for your bounding box dimensions.

